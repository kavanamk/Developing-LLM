{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define key variables used in the code for GAN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_DIM = 1\n",
    "G_HIDDEN = 10\n",
    "X_DIM = 10\n",
    "D_HIDDEN = 10\n",
    "\n",
    "step_size_G = 0.01\n",
    "step_size_D = 0.01\n",
    "ITER_NUM = 50000\n",
    "\n",
    "GRADIENT_CLIP = 0.2\n",
    "WEIGHT_CLIP = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define functions and initializer for GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(random=True):\n",
    "    if random:\n",
    "        x0 = np.random.uniform(0, 1)\n",
    "        freq = np.random.uniform(1.2, 1.5)\n",
    "        mult = np.random.uniform(0.5, 0.8)\n",
    "    else:\n",
    "        x0 = 0\n",
    "        freq = 0.2\n",
    "        mult = 1\n",
    "    signal = [mult * np.sin(x0+freq*i) for i in range(X_DIM)]\n",
    "    return np.array(signal)\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(x, 0.)\n",
    "\n",
    "\n",
    "def dReLU(x):\n",
    "    return ReLU(x)\n",
    "\n",
    "\n",
    "def LeakyReLU(x, k=0.2):\n",
    "    return np.where(x >= 0, x, x * k)\n",
    "\n",
    "\n",
    "def dLeakyReLU(x, k=0.2):\n",
    "    return np.where(x >= 0, 1., k)\n",
    "\n",
    "\n",
    "def Tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dTanh(x):\n",
    "    return 1. - Tanh(x)**2\n",
    "\n",
    "\n",
    "def Sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "\n",
    "def dSigmoid(x):\n",
    "    return Sigmoid(x) * (1. - Sigmoid(x))\n",
    "\n",
    "\n",
    "def weight_initializer(in_channels, out_channels):\n",
    "    scale = np.sqrt(2. / (in_channels + out_channels))\n",
    "    return np.random.uniform(-scale, scale, (in_channels, out_channels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the activation functions and their derivatives. If you're not familiar with the concept of activation functions, just remember that their jobs are to adjust the outputs of a layer so that its next layer can have a better understanding of these output values:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunc(object):\n",
    "    def __init__(self):\n",
    "        self.logit = None\n",
    "        self.label = None\n",
    "\n",
    "    def forward(self, logit, label):\n",
    "        if logit[0, 0] < 1e-7:\n",
    "            logit[0, 0] = 1e-7\n",
    "        if 1. - logit[0, 0] < 1e-7:\n",
    "            logit[0, 0] = 1. - 1e-7\n",
    "        self.logit = logit\n",
    "        self.label = label\n",
    "        return - (label * np.log(logit) + (1-label) * np.log(1-logit))\n",
    "\n",
    "    def backward(self):\n",
    "        return (1-self.label) / (1-self.logit) - self.label / self.logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Define generator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(object):\n",
    "    def __init__(self):\n",
    "        self.z = None\n",
    "        self.w1 = weight_initializer(Z_DIM, G_HIDDEN)\n",
    "        self.b1 = weight_initializer(1, G_HIDDEN)\n",
    "        self.x1 = None\n",
    "        self.w2 = weight_initializer(G_HIDDEN, G_HIDDEN)\n",
    "        self.b2 = weight_initializer(1, G_HIDDEN)\n",
    "        self.x2 = None\n",
    "        self.w3 = weight_initializer(G_HIDDEN, X_DIM)\n",
    "        self.b3 = weight_initializer(1, X_DIM)\n",
    "        self.x3 = None\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.z = inputs.reshape(1, Z_DIM)\n",
    "        self.x1 = np.matmul(self.z, self.w1) + self.b1\n",
    "        self.x1 = ReLU(self.x1)\n",
    "        self.x2 = np.matmul(self.x1, self.w2) + self.b2\n",
    "        self.x2 = ReLU(self.x2)\n",
    "        self.x3 = np.matmul(self.x2, self.w3) + self.b3\n",
    "        self.x = Tanh(self.x3)\n",
    "        return self.x\n",
    "\n",
    "    def backward(self, outputs):\n",
    "        # Derivative w.r.t. output\n",
    "        delta = outputs\n",
    "        delta *= dTanh(self.x)\n",
    "        # Derivative w.r.t. w3\n",
    "        d_w3 = np.matmul(np.transpose(self.x2), delta)\n",
    "        # Derivative w.r.t. b3\n",
    "        d_b3 = delta.copy()\n",
    "        # Derivative w.r.t. x2\n",
    "        delta = np.matmul(delta, np.transpose(self.w3))\n",
    "        # Update w3\n",
    "        if (np.linalg.norm(d_w3) > GRADIENT_CLIP):\n",
    "            d_w3 = GRADIENT_CLIP / np.linalg.norm(d_w3) * d_w3\n",
    "        self.w3 -= step_size_G * d_w3\n",
    "        self.w3 = np.maximum(-WEIGHT_CLIP, np.minimum(WEIGHT_CLIP, self.w3))\n",
    "        # Update b3\n",
    "        self.b3 -= step_size_G * d_b3\n",
    "        self.b3 = np.maximum(-WEIGHT_CLIP, np.minimum(WEIGHT_CLIP, self.b3))\n",
    "        delta *= dReLU(self.x2)\n",
    "        # Derivative w.r.t. w2\n",
    "        d_w2 = np.matmul(np.transpose(self.x1), delta)\n",
    "        # Derivative w.r.t. b2\n",
    "        d_b2 = delta.copy()\n",
    "\n",
    "        # Derivative w.r.t. x1\n",
    "        delta = np.matmul(delta, np.transpose(self.w2))\n",
    "\n",
    "        # Update w2\n",
    "        if (np.linalg.norm(d_w2) > GRADIENT_CLIP):\n",
    "            d_w2 = GRADIENT_CLIP / np.linalg.norm(d_w2) * d_w2\n",
    "        self.w2 -= step_size_G * d_w2\n",
    "        self.w2 = np.maximum(-WEIGHT_CLIP, np.minimum(WEIGHT_CLIP, self.w2))\n",
    "\n",
    "        # Update b2\n",
    "        self.b2 -= step_size_G * d_b2\n",
    "        self.b2 = np.maximum(-WEIGHT_CLIP, np.minimum(WEIGHT_CLIP, self.b2))\n",
    "        delta *= dReLU(self.x1)\n",
    "        # Derivative w.r.t. w1\n",
    "        d_w1 = np.matmul(np.transpose(self.z), delta)\n",
    "        # Derivative w.r.t. b1\n",
    "        d_b1 = delta.copy()\n",
    "\n",
    "        # No need to calculate derivative w.r.t. z\n",
    "        # Update w1\n",
    "        if (np.linalg.norm(d_w1) > GRADIENT_CLIP):\n",
    "            d_w1 = GRADIENT_CLIP / np.linalg.norm(d_w1) * d_w1\n",
    "        self.w1 -= step_size_G * d_w1\n",
    "        self.w1 = np.maximum(-WEIGHT_CLIP, np.minimum(WEIGHT_CLIP, self.w1))\n",
    "\n",
    "        # Update b1\n",
    "        self.b1 -= step_size_G * d_b1\n",
    "        self.b1 = np.maximum(-WEIGHT_CLIP, np.minimum(WEIGHT_CLIP, self.b1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Define discriminator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Discriminator(object):\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.w1 = weight_initializer(X_DIM, D_HIDDEN)\n",
    "        self.b1 = weight_initializer(1, D_HIDDEN)\n",
    "        self.y1 = None\n",
    "        self.w2 = weight_initializer(D_HIDDEN, D_HIDDEN)\n",
    "        self.b2 = weight_initializer(1, D_HIDDEN)\n",
    "        self.y2 = None\n",
    "        self.w3 = weight_initializer(D_HIDDEN, 1)\n",
    "        self.b3 = weight_initializer(1, 1)\n",
    "        self.y3 = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.x = inputs.reshape(1, X_DIM)\n",
    "        self.y1 = np.matmul(self.x, self.w1) + self.b1\n",
    "        self.y1 = LeakyReLU(self.y1)\n",
    "        self.y2 = np.matmul(self.y1, self.w2) + self.b2\n",
    "        self.y2 = LeakyReLU(self.y2)\n",
    "        self.y3 = np.matmul(self.y2, self.w3) + self.b3\n",
    "        self.y = Sigmoid(self.y3)\n",
    "        return self.y\n",
    "\n",
    "    def backward(self, outputs, apply_grads=True):\n",
    "        # Derivative w.r.t. output\n",
    "        delta = outputs\n",
    "        delta *= dSigmoid(self.y)\n",
    "        # Derivative w.r.t. w3\n",
    "        d_w3 = np.matmul(np.transpose(self.y2), delta)\n",
    "        # Derivative w.r.t. b3\n",
    "        d_b3 = delta.copy()\n",
    "        # Derivative w.r.t. y2\n",
    "        delta = np.matmul(delta, np.transpose(self.w3))\n",
    "        if apply_grads:\n",
    "            # Update w3\n",
    "            if np.linalg.norm(d_w3) > GRADIENT_CLIP:\n",
    "                d_w3 = GRADIENT_CLIP / np.linalg.norm(d_w3) * d_w3\n",
    "            self.w3 += step_size_D * d_w3\n",
    "            self.w3 = np.maximum(-WEIGHT_CLIP,\n",
    "                                 np.minimum(WEIGHT_CLIP, self.w3))\n",
    "            # Update b3\n",
    "            self.b3 += step_size_D * d_b3\n",
    "            self.b3 = np.maximum(-WEIGHT_CLIP,\n",
    "                                 np.minimum(WEIGHT_CLIP, self.b3))\n",
    "        delta *= dLeakyReLU(self.y2)\n",
    "        # Derivative w.r.t. w2\n",
    "        d_w2 = np.matmul(np.transpose(self.y1), delta)\n",
    "        # Derivative w.r.t. b2\n",
    "        d_b2 = delta.copy()\n",
    "        # Derivative w.r.t. y1\n",
    "        delta = np.matmul(delta, np.transpose(self.w2))\n",
    "        if apply_grads:\n",
    "            # Update w2\n",
    "            if np.linalg.norm(d_w2) > GRADIENT_CLIP:\n",
    "                d_w2 = GRADIENT_CLIP / np.linalg.norm(d_w2) * d_w2\n",
    "            self.w2 += step_size_D * d_w2\n",
    "            self.w2 = np.maximum(-WEIGHT_CLIP,\n",
    "                                 np.minimum(WEIGHT_CLIP, self.w2))\n",
    "            # Update b2\n",
    "            self.b2 += step_size_D * d_b2\n",
    "            self.b2 = np.maximum(-WEIGHT_CLIP,\n",
    "                                 np.minimum(WEIGHT_CLIP, self.b2))\n",
    "        delta *= dLeakyReLU(self.y1)\n",
    "        # Derivative w.r.t. w1\n",
    "        d_w1 = np.matmul(np.transpose(self.x), delta)\n",
    "        # Derivative w.r.t. b1\n",
    "        d_b1 = delta.copy()\n",
    "        # Derivative w.r.t. x\n",
    "        delta = np.matmul(delta, np.transpose(self.w1))\n",
    "        # Update w1\n",
    "        if apply_grads:\n",
    "            if np.linalg.norm(d_w1) > GRADIENT_CLIP:\n",
    "                d_w1 = GRADIENT_CLIP / np.linalg.norm(d_w1) * d_w1\n",
    "            self.w1 += step_size_D * d_w1\n",
    "            self.w1 = np.maximum(-WEIGHT_CLIP,\n",
    "                                 np.minimum(WEIGHT_CLIP, self.w1))\n",
    "            # Update b1\n",
    "            self.b1 += step_size_D * d_b1\n",
    "            self.b1 = np.maximum(-WEIGHT_CLIP,\n",
    "                                 np.minimum(WEIGHT_CLIP, self.b1))\n",
    "        return delta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Create Generator and Discriminator objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator()\n",
    "D = Discriminator()\n",
    "criterion = LossFunc()\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Create training loop for GAN and generating data graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for itr in range(ITER_NUM):\n",
    "    # Update D with real data\n",
    "    x_real = get_samples(True)\n",
    "    y_real = D.forward(x_real)\n",
    "    loss_D_r = criterion.forward(y_real, real_label)\n",
    "    d_loss_D = criterion.backward()\n",
    "    D.backward(d_loss_D)\n",
    "\n",
    "    # Update D with fake data\n",
    "    z_noise = np.random.randn(Z_DIM)\n",
    "    x_fake = G.forward(z_noise)\n",
    "    y_fake = D.forward(x_fake)\n",
    "    loss_D_f = criterion.forward(y_fake, fake_label)\n",
    "    d_loss_D = criterion.backward()\n",
    "    D.backward(d_loss_D)\n",
    "\n",
    "    # Update G with fake data\n",
    "    y_fake_r = D.forward(x_fake)\n",
    "    loss_G = criterion.forward(y_fake_r, real_label)\n",
    "    d_loss_G = D.backward(loss_G, apply_grads=False)\n",
    "    G.backward(d_loss_G)\n",
    "    loss_D = loss_D_r + loss_D_f\n",
    "    if itr % 100 == 0:\n",
    "        print('{} {} {}'.format(loss_D_r.item((0, 0)),\n",
    "                                loss_D_f.item((0, 0)), loss_G.item((0, 0))))\n",
    "\n",
    "x_axis = np.linspace(0, 10, 10)\n",
    "for i in range(50):\n",
    "    z_noise = np.random.randn(Z_DIM)\n",
    "    x_fake = G.forward(z_noise)\n",
    "    plt.plot(x_axis, x_fake.reshape(X_DIM))\n",
    "plt.ylim((-1, 1))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
